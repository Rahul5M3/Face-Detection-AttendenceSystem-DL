{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model   \n",
    "from tensorflow.keras.layers import Input, Conv2D, Dense, GlobalMaxPooling2D\n",
    "from tensorflow.keras.applications import VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "\u001b[1m58889256/58889256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 2us/step\n"
     ]
    }
   ],
   "source": [
    "vgg= VGG16(include_top=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vgg.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_build():\n",
    "    input_layer=Input(shape=(120,120,3))\n",
    "    vgg=VGG16(include_top=False)(input_layer)\n",
    "\n",
    "    classif=GlobalMaxPooling2D()(vgg)\n",
    "    class1=Dense(2000, activation='relu')(classif)\n",
    "    class2=Dense(1, activation='sigmoid')(class1)\n",
    "\n",
    "    regress=GlobalMaxPooling2D()(vgg)\n",
    "    reg1=Dense(2000, activation='relu')(regress)\n",
    "    reg2=Dense(4, activation='sigmoid')(reg1)\n",
    "\n",
    "    faceDetector=Model(inputs=input_layer, outputs=[class2, reg2])\n",
    "    return faceDetector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "faceDetector=model_build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_1       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">120</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">120</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ vgg16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>) │ <span style=\"color: #00af00; text-decoration-color: #00af00\">14,714,688</span> │ input_layer_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ global_max_pooling… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ vgg16[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling2…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ global_max_pooling… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ vgg16[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling2…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2000</span>)      │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,026,000</span> │ global_max_pooli… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2000</span>)      │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,026,000</span> │ global_max_pooli… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,001</span> │ dense[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)         │      <span style=\"color: #00af00; text-decoration-color: #00af00\">8,004</span> │ dense_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_1       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m120\u001b[0m, \u001b[38;5;34m120\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │ \u001b[38;5;34m3\u001b[0m)                │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ vgg16 (\u001b[38;5;33mFunctional\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m512\u001b[0m) │ \u001b[38;5;34m14,714,688\u001b[0m │ input_layer_1[\u001b[38;5;34m0\u001b[0m]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ global_max_pooling… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ vgg16[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "│ (\u001b[38;5;33mGlobalMaxPooling2…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ global_max_pooling… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ vgg16[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "│ (\u001b[38;5;33mGlobalMaxPooling2…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2000\u001b[0m)      │  \u001b[38;5;34m1,026,000\u001b[0m │ global_max_pooli… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2000\u001b[0m)      │  \u001b[38;5;34m1,026,000\u001b[0m │ global_max_pooli… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │      \u001b[38;5;34m2,001\u001b[0m │ dense[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m)         │      \u001b[38;5;34m8,004\u001b[0m │ dense_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">16,776,693</span> (64.00 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m16,776,693\u001b[0m (64.00 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">16,776,693</span> (64.00 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m16,776,693\u001b[0m (64.00 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "faceDetector.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "train=tf.data.Dataset.load('../saved_train')\n",
    "val=tf.data.Dataset.load('../saved_val')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y=train.as_numpy_iterator().next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 120, 120, 3)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n"
     ]
    }
   ],
   "source": [
    "classes, coordinates=faceDetector.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches_per_epoch=len(train)\n",
    "initial_learning_rate=0.0001\n",
    "decay_steps=batches_per_epoch\n",
    "decay_rate=0.9\n",
    "\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate,\n",
    "    decay_steps=decay_steps,\n",
    "    decay_rate=decay_rate,\n",
    "    staircase=True  # If True, decay happens at discrete intervals\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt=tf.keras.optimizers.Adam(learning_rate=lr_schedule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_localization_loss(yTrue, yPred):\n",
    "    coordinate=tf.reduce_sum(tf.square(yTrue[:,:2]-yPred[:,:2]))\n",
    "    heightTrue=yTrue[:,3]-yTrue[:,1]\n",
    "    widthTrue=yTrue[:,2]-yTrue[:,0]\n",
    "\n",
    "    heightPred=yPred[:,3]-yPred[:,1]\n",
    "    widthPred=yPred[:,2]-yPred[:,0]\n",
    "\n",
    "    size=tf.reduce_sum(tf.square(widthTrue-widthPred) + tf.square(heightTrue-heightPred))\n",
    "\n",
    "    return coordinate+size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "classLoss=tf.keras.losses.BinaryCrossentropy()\n",
    "regressLoss=compute_localization_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute_localization_loss(y[1], coordinates)\n",
    "# classLoss(y[0],classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X: (10, 120, 120, 3)\n",
      "Shape of y[0] (class labels): (10, 1)\n",
      "Shape of y[1] (bounding boxes): (10, 4)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of X:\", X.shape)  # Should print (batch_size, 120, 120, 3)\n",
    "print(\"Shape of y[0] (class labels):\", y[0].shape)  # Should print (batch_size, num_classes)\n",
    "print(\"Shape of y[1] (bounding boxes):\", y[1].shape)  # Should print (batch_size, num_boxes, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaceDetector(Model):\n",
    "    def __init__(self, faceDetector, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.model=faceDetector\n",
    "\n",
    "    def compile(self, opt, classLoss, regressLoss, **kwargs):\n",
    "            # super().compile(**kwargs)\n",
    "            # self.opt=opt\n",
    "            # self.cLoss=classLoss\n",
    "            # self.lLoss=regressLoss\n",
    "\n",
    "            super().compile(\n",
    "                  optimizer=opt,\n",
    "                  loss=[classLoss,regressLoss],\n",
    "                  **kwargs\n",
    "            )\n",
    "            self.opt=opt\n",
    "            self.cLoss=classLoss\n",
    "            self.lLoss=regressLoss\n",
    "\n",
    "            # self.cLoss=loss.get('classification')\n",
    "            # self.lLoss=loss.get('regression')\n",
    "            # super().compile(optimizer=opt, loss={'classification': self.cLoss, 'regression': self.lLoss},**kwargs)\n",
    "            # self.opt=opt\n",
    "\n",
    "    def train_step(self, batch, **kwargs):\n",
    "        X,y=batch\n",
    "        # Ensure the shape of X is (batch_size, 120, 120, 3)\n",
    "        X = tf.ensure_shape(X, [None, 120, 120, 3])\n",
    "\n",
    "        y = list(y)\n",
    "\n",
    "        # # Ensure y has the expected shape (batch_size, num_classes) for class labels\n",
    "        y[0] = tf.ensure_shape(y[0], [None, 1])  # Adjust num_classes as needed\n",
    "        y[1] = tf.ensure_shape(y[1], [None, 4])  # Adjust num_boxes as needed\n",
    "\n",
    "        y = tuple(y)\n",
    "\n",
    "\n",
    "        # print(f\"Rank of X: {tf.rank(X)}\")\n",
    "        # print(f\"Rank of y: {tf.rank(y)}\")\n",
    "        with tf.GradientTape()  as tape:\n",
    "              classes,coordinates= self.model(X,training=True)\n",
    "              classLoss=self.cLoss(y[0],classes)\n",
    "              localizationLoss=self.lLoss(tf.cast(y[1],tf.float32),coordinates)\n",
    "              totalLoss=0.5*classLoss+localizationLoss\n",
    "              grad=tape.gradient(totalLoss,self.model.trainable_variables)\n",
    "\n",
    "        opt.apply_gradients(zip(grad,self.model.trainable_variables))   \n",
    "        return {'totalLoss':totalLoss, 'classLoss':classLoss, 'regressLoss':localizationLoss}\n",
    "\n",
    "    def test_step(self, batch, **knwargs):\n",
    "              \n",
    "            X,y=batch\n",
    "\n",
    "            # Ensure the shape of X is (batch_size, 120, 120, 3)\n",
    "            X = tf.ensure_shape(X, [None, 120, 120, 3])\n",
    "\n",
    "            y = list(y)\n",
    "\n",
    "            # # Ensure y has the expected shape (batch_size, num_classes) for class labels\n",
    "            y[0] = tf.ensure_shape(y[0], [None, 1])  # Adjust num_classes as needed\n",
    "            y[1] = tf.ensure_shape(y[1], [None, 4])  # Adjust num_boxes as needed\n",
    "\n",
    "            y = tuple(y)\n",
    "\n",
    "            classes,coordinates= self.model(X,training=True)\n",
    "            classLoss=self.cLoss(y[0],classes)\n",
    "            localizationLoss=self.lLoss(tf.cast(y[1],tf.float32),coordinates)\n",
    "            totalLoss=0.5*classLoss+localizationLoss\n",
    "\n",
    "            return {'totalLoss':totalLoss, 'classLoss':classLoss, 'regressLoss':localizationLoss}\n",
    "    \n",
    "    def call(self, X, **kwargs):\n",
    "          return self.model(X, **kwargs)\n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=FaceDetector(faceDetector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(opt, classLoss, regressLoss)\n",
    "# model.compile(opt, loss={'classification': classLoss, 'regression': regressLoss})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "logDir='../visulaizationData'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorBoard=tf.keras.callbacks.TensorBoard(log_dir=logDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m116s\u001b[0m 2s/step - classLoss: 0.0499 - regressLoss: 0.2596 - totalLoss: 0.2845 - val_classLoss: 0.0031 - val_regressLoss: 0.0963 - val_totalLoss: 0.0979\n",
      "Epoch 2/30\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 2s/step - classLoss: 0.0286 - regressLoss: 0.1783 - totalLoss: 0.1926 - val_classLoss: 0.0010 - val_regressLoss: 0.0646 - val_totalLoss: 0.0651\n",
      "Epoch 3/30\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m115s\u001b[0m 2s/step - classLoss: 0.0267 - regressLoss: 0.1379 - totalLoss: 0.1513 - val_classLoss: 0.0031 - val_regressLoss: 0.0285 - val_totalLoss: 0.0300\n",
      "Epoch 4/30\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 2s/step - classLoss: 0.0292 - regressLoss: 0.1174 - totalLoss: 0.1321 - val_classLoss: 0.0024 - val_regressLoss: 0.0529 - val_totalLoss: 0.0541\n",
      "Epoch 5/30\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 2s/step - classLoss: 0.0130 - regressLoss: 0.0753 - totalLoss: 0.0818 - val_classLoss: 3.4726e-04 - val_regressLoss: 0.0221 - val_totalLoss: 0.0223\n",
      "Epoch 6/30\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 2s/step - classLoss: 0.0031 - regressLoss: 0.0274 - totalLoss: 0.0289 - val_classLoss: 2.0995e-04 - val_regressLoss: 0.0298 - val_totalLoss: 0.0299\n",
      "Epoch 7/30\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 2s/step - classLoss: 0.0113 - regressLoss: 0.0492 - totalLoss: 0.0548 - val_classLoss: 1.7030e-04 - val_regressLoss: 0.0242 - val_totalLoss: 0.0243\n",
      "Epoch 8/30\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 1s/step - classLoss: 0.0055 - regressLoss: 0.0245 - totalLoss: 0.0272 - val_classLoss: 2.1980e-04 - val_regressLoss: 0.0237 - val_totalLoss: 0.0238\n",
      "Epoch 9/30\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 1s/step - classLoss: 5.7239e-04 - regressLoss: 0.0105 - totalLoss: 0.0108 - val_classLoss: 2.6240e-04 - val_regressLoss: 0.0153 - val_totalLoss: 0.0154\n",
      "Epoch 10/30\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 1s/step - classLoss: 3.6819e-04 - regressLoss: 0.0078 - totalLoss: 0.0080 - val_classLoss: 2.3936e-04 - val_regressLoss: 0.0145 - val_totalLoss: 0.0146\n",
      "Epoch 11/30\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 1s/step - classLoss: 2.9648e-04 - regressLoss: 0.0066 - totalLoss: 0.0068 - val_classLoss: 2.1254e-04 - val_regressLoss: 0.0147 - val_totalLoss: 0.0148\n",
      "Epoch 12/30\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 1s/step - classLoss: 2.4891e-04 - regressLoss: 0.0059 - totalLoss: 0.0060 - val_classLoss: 1.8443e-04 - val_regressLoss: 0.0149 - val_totalLoss: 0.0150\n",
      "Epoch 13/30\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 1s/step - classLoss: 2.1511e-04 - regressLoss: 0.0052 - totalLoss: 0.0053 - val_classLoss: 1.5793e-04 - val_regressLoss: 0.0151 - val_totalLoss: 0.0151\n",
      "Epoch 14/30\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 1s/step - classLoss: 1.8999e-04 - regressLoss: 0.0046 - totalLoss: 0.0047 - val_classLoss: 1.3883e-04 - val_regressLoss: 0.0147 - val_totalLoss: 0.0148\n",
      "Epoch 15/30\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 1s/step - classLoss: 1.6891e-04 - regressLoss: 0.0042 - totalLoss: 0.0043 - val_classLoss: 1.2603e-04 - val_regressLoss: 0.0146 - val_totalLoss: 0.0146\n",
      "Epoch 16/30\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 1s/step - classLoss: 1.5301e-04 - regressLoss: 0.0040 - totalLoss: 0.0041 - val_classLoss: 1.1408e-04 - val_regressLoss: 0.0153 - val_totalLoss: 0.0154\n",
      "Epoch 17/30\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 1s/step - classLoss: 1.4080e-04 - regressLoss: 0.0040 - totalLoss: 0.0040 - val_classLoss: 1.0209e-04 - val_regressLoss: 0.0168 - val_totalLoss: 0.0168\n",
      "Epoch 18/30\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 1s/step - classLoss: 1.3186e-04 - regressLoss: 0.0040 - totalLoss: 0.0040 - val_classLoss: 9.3544e-05 - val_regressLoss: 0.0180 - val_totalLoss: 0.0180\n",
      "Epoch 19/30\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 1s/step - classLoss: 1.2478e-04 - regressLoss: 0.0037 - totalLoss: 0.0038 - val_classLoss: 8.8427e-05 - val_regressLoss: 0.0173 - val_totalLoss: 0.0173\n",
      "Epoch 20/30\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 1s/step - classLoss: 1.1848e-04 - regressLoss: 0.0033 - totalLoss: 0.0033 - val_classLoss: 8.3237e-05 - val_regressLoss: 0.0155 - val_totalLoss: 0.0155\n",
      "Epoch 21/30\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 1s/step - classLoss: 1.1081e-04 - regressLoss: 0.0028 - totalLoss: 0.0029 - val_classLoss: 7.8264e-05 - val_regressLoss: 0.0148 - val_totalLoss: 0.0148\n",
      "Epoch 22/30\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 1s/step - classLoss: 1.0370e-04 - regressLoss: 0.0025 - totalLoss: 0.0025 - val_classLoss: 7.3452e-05 - val_regressLoss: 0.0147 - val_totalLoss: 0.0148\n",
      "Epoch 23/30\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 1s/step - classLoss: 9.7697e-05 - regressLoss: 0.0023 - totalLoss: 0.0023 - val_classLoss: 7.0481e-05 - val_regressLoss: 0.0143 - val_totalLoss: 0.0143\n",
      "Epoch 24/30\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 1s/step - classLoss: 9.2851e-05 - regressLoss: 0.0022 - totalLoss: 0.0022 - val_classLoss: 6.8505e-05 - val_regressLoss: 0.0135 - val_totalLoss: 0.0136\n",
      "Epoch 25/30\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 1s/step - classLoss: 8.8756e-05 - regressLoss: 0.0021 - totalLoss: 0.0022 - val_classLoss: 6.7071e-05 - val_regressLoss: 0.0126 - val_totalLoss: 0.0127\n",
      "Epoch 26/30\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 1s/step - classLoss: 8.5270e-05 - regressLoss: 0.0021 - totalLoss: 0.0022 - val_classLoss: 6.5927e-05 - val_regressLoss: 0.0117 - val_totalLoss: 0.0117\n",
      "Epoch 27/30\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 1s/step - classLoss: 8.2460e-05 - regressLoss: 0.0021 - totalLoss: 0.0021 - val_classLoss: 6.5320e-05 - val_regressLoss: 0.0110 - val_totalLoss: 0.0110\n",
      "Epoch 28/30\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 1s/step - classLoss: 8.0068e-05 - regressLoss: 0.0021 - totalLoss: 0.0021 - val_classLoss: 6.4599e-05 - val_regressLoss: 0.0106 - val_totalLoss: 0.0106\n",
      "Epoch 29/30\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 1s/step - classLoss: 7.8022e-05 - regressLoss: 0.0020 - totalLoss: 0.0021 - val_classLoss: 6.3465e-05 - val_regressLoss: 0.0105 - val_totalLoss: 0.0105\n",
      "Epoch 30/30\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 1s/step - classLoss: 7.6184e-05 - regressLoss: 0.0019 - totalLoss: 0.0020 - val_classLoss: 6.1859e-05 - val_regressLoss: 0.0107 - val_totalLoss: 0.0107\n"
     ]
    }
   ],
   "source": [
    "hist=model.fit(train, epochs=30, validation_data=val, callbacks=[tensorBoard])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow.python.data.ops.load_op._LoadDataset"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(10, 120, 120, 3), dtype=float32, numpy=\n",
      "array([[[[0.3807598 , 0.38811275, 0.33063725],\n",
      "         [0.40765932, 0.37922794, 0.34834558],\n",
      "         [0.4266544 , 0.37224266, 0.33694854],\n",
      "         ...,\n",
      "         [0.47205883, 0.4127451 , 0.33970588],\n",
      "         [0.47879902, 0.4317402 , 0.37193626],\n",
      "         [0.45637256, 0.43137255, 0.3495098 ]],\n",
      "\n",
      "        [[0.41832107, 0.42273283, 0.4041054 ],\n",
      "         [0.41556373, 0.3959559 , 0.3788603 ],\n",
      "         [0.43229166, 0.39442402, 0.3552696 ],\n",
      "         ...,\n",
      "         [0.46415442, 0.41317403, 0.3504289 ],\n",
      "         [0.46801472, 0.41911766, 0.37199754],\n",
      "         [0.46519607, 0.41715688, 0.36531863]],\n",
      "\n",
      "        [[0.425     , 0.4708946 , 0.46746323],\n",
      "         [0.41458333, 0.43131128, 0.4033701 ],\n",
      "         [0.42990196, 0.4028799 , 0.36452207],\n",
      "         ...,\n",
      "         [0.4543505 , 0.4033701 , 0.3362745 ],\n",
      "         [0.47591913, 0.41060048, 0.33970588],\n",
      "         [0.48112744, 0.4146446 , 0.34503677]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.46372548, 0.5892157 , 0.6480392 ],\n",
      "         [0.49417892, 0.61623776, 0.68535537],\n",
      "         [0.48639706, 0.6009804 , 0.6714461 ],\n",
      "         ...,\n",
      "         [0.8560049 , 0.96384805, 0.9442402 ],\n",
      "         [0.80631125, 0.9469975 , 0.93131125],\n",
      "         [0.78792894, 0.9370098 , 0.9310049 ]],\n",
      "\n",
      "        [[0.4490196 , 0.5857843 , 0.64215684],\n",
      "         [0.5121936 , 0.62199754, 0.66905636],\n",
      "         [0.5314338 , 0.62352943, 0.6779412 ],\n",
      "         ...,\n",
      "         [0.8258578 , 0.96525735, 0.95      ],\n",
      "         [0.8027574 , 0.94393384, 0.9351103 ],\n",
      "         [0.8102328 , 0.95140934, 0.93572307]],\n",
      "\n",
      "        [[0.4622549 , 0.60159314, 0.6623162 ],\n",
      "         [0.4913603 , 0.5957721 , 0.6393995 ],\n",
      "         [0.49773285, 0.58345586, 0.6346201 ],\n",
      "         ...,\n",
      "         [0.7696691 , 0.93376225, 0.92653185],\n",
      "         [0.78884804, 0.94197303, 0.9311887 ],\n",
      "         [0.83204657, 0.9643995 , 0.9384191 ]]],\n",
      "\n",
      "\n",
      "       [[[0.68688726, 0.91433823, 0.8643382 ],\n",
      "         [0.6713848 , 0.90569854, 0.83020836],\n",
      "         [0.6745098 , 0.9166667 , 0.85183823],\n",
      "         ...,\n",
      "         [0.5158701 , 0.73333335, 0.65539217],\n",
      "         [0.43155637, 0.6942402 , 0.6186274 ],\n",
      "         [0.41421568, 0.6696078 , 0.6048407 ]],\n",
      "\n",
      "        [[0.6696078 , 0.904902  , 0.8699755 ],\n",
      "         [0.66072303, 0.9038603 , 0.8518995 ],\n",
      "         [0.7137868 , 0.96476716, 0.9015319 ],\n",
      "         ...,\n",
      "         [0.54221815, 0.747549  , 0.65545344],\n",
      "         [0.4882353 , 0.72794116, 0.6377451 ],\n",
      "         [0.43039215, 0.6800245 , 0.60851717]],\n",
      "\n",
      "        [[0.6754289 , 0.9028799 , 0.85153186],\n",
      "         [0.69460785, 0.93039215, 0.88186276],\n",
      "         [0.68419117, 0.93210787, 0.8503064 ],\n",
      "         ...,\n",
      "         [0.49607843, 0.7064951 , 0.611826  ],\n",
      "         [0.50784314, 0.7230392 , 0.63235295],\n",
      "         [0.44699755, 0.68688726, 0.59767157]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.38878676, 0.40980393, 0.21164216],\n",
      "         [0.38186276, 0.4057598 , 0.22738971],\n",
      "         [0.39908087, 0.4186887 , 0.2579044 ],\n",
      "         ...,\n",
      "         [0.4014706 , 0.39362746, 0.19448529],\n",
      "         [0.39503676, 0.38376224, 0.20582108],\n",
      "         [0.36911765, 0.36292893, 0.17830883]],\n",
      "\n",
      "        [[0.3572304 , 0.38762254, 0.17732844],\n",
      "         [0.3456495 , 0.39607844, 0.21672794],\n",
      "         [0.34381127, 0.37561274, 0.22867647],\n",
      "         ...,\n",
      "         [0.3622549 , 0.3612745 , 0.15784314],\n",
      "         [0.36813724, 0.36421567, 0.18970588],\n",
      "         [0.36409312, 0.36795342, 0.19705883]],\n",
      "\n",
      "        [[0.36911765, 0.3930147 , 0.19895834],\n",
      "         [0.3425245 , 0.40545344, 0.22996323],\n",
      "         [0.31256127, 0.3552696 , 0.21280637],\n",
      "         ...,\n",
      "         [0.35606617, 0.36133578, 0.16525735],\n",
      "         [0.35667893, 0.36825982, 0.19080882],\n",
      "         [0.37696078, 0.3887255 , 0.21593137]]],\n",
      "\n",
      "\n",
      "       [[[0.2055147 , 0.1726103 , 0.11090686],\n",
      "         [0.18627451, 0.16323529, 0.09460784],\n",
      "         [0.2615196 , 0.2329044 , 0.17457108],\n",
      "         ...,\n",
      "         [0.2998162 , 0.2591299 , 0.22824755],\n",
      "         [0.30998775, 0.25900736, 0.23547794],\n",
      "         [0.2987745 , 0.24963236, 0.22610295]],\n",
      "\n",
      "        [[0.16102941, 0.12671569, 0.0997549 ],\n",
      "         [0.2646446 , 0.2254902 , 0.19258578],\n",
      "         [0.2627451 , 0.22303921, 0.18921569],\n",
      "         ...,\n",
      "         [0.30042893, 0.2651348 , 0.23768382],\n",
      "         [0.2769608 , 0.23480392, 0.20992647],\n",
      "         [0.26930147, 0.23890932, 0.20802696]],\n",
      "\n",
      "        [[0.18762255, 0.16078432, 0.13039216],\n",
      "         [0.26715687, 0.23921569, 0.21715686],\n",
      "         [0.25422794, 0.22677696, 0.20324755],\n",
      "         ...,\n",
      "         [0.2509191 , 0.22346814, 0.1920956 ],\n",
      "         [0.27598038, 0.2485294 , 0.20931372],\n",
      "         [0.28118873, 0.2574755 , 0.21078432]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.2981005 , 0.38253677, 0.45968136],\n",
      "         [0.28235295, 0.3887255 , 0.4642157 ],\n",
      "         [0.27450982, 0.39607844, 0.4745098 ],\n",
      "         ...,\n",
      "         [0.47935048, 0.5813113 , 0.63829654],\n",
      "         [0.48278186, 0.5925858 , 0.6455882 ],\n",
      "         [0.47205883, 0.5897059 , 0.63235295]],\n",
      "\n",
      "        [[0.28835785, 0.3893995 , 0.4495098 ],\n",
      "         [0.27751225, 0.38431373, 0.45251226],\n",
      "         [0.26911765, 0.39068627, 0.4622549 ],\n",
      "         ...,\n",
      "         [0.4848652 , 0.59074754, 0.6338848 ],\n",
      "         [0.4788603 , 0.58572304, 0.62052697],\n",
      "         [0.46801472, 0.58566177, 0.61740196]],\n",
      "\n",
      "        [[0.29319853, 0.40300244, 0.45006126],\n",
      "         [0.2954044 , 0.4079657 , 0.45196077],\n",
      "         [0.28382352, 0.40110293, 0.46158087],\n",
      "         ...,\n",
      "         [0.48517156, 0.5729167 , 0.62781864],\n",
      "         [0.47616422, 0.5675245 , 0.6194853 ],\n",
      "         [0.46801472, 0.56262255, 0.6209559 ]]],\n",
      "\n",
      "\n",
      "       ...,\n",
      "\n",
      "\n",
      "       [[[0.5420956 , 0.4768995 , 0.39405638],\n",
      "         [0.53639704, 0.48578432, 0.3816789 ],\n",
      "         [0.5480392 , 0.4826593 , 0.3617647 ],\n",
      "         ...,\n",
      "         [0.35514706, 0.31170344, 0.27555147],\n",
      "         [0.36452207, 0.30061275, 0.2566789 ],\n",
      "         [0.35104167, 0.27506128, 0.24356617]],\n",
      "\n",
      "        [[0.55655634, 0.4927696 , 0.41470587],\n",
      "         [0.5372549 , 0.47751224, 0.3863358 ],\n",
      "         [0.53743875, 0.47028187, 0.36930147],\n",
      "         ...,\n",
      "         [0.40765932, 0.33272058, 0.27689952],\n",
      "         [0.40018383, 0.33339462, 0.26286766],\n",
      "         [0.35257354, 0.28345588, 0.22561274]],\n",
      "\n",
      "        [[0.5808824 , 0.5181373 , 0.3810049 ],\n",
      "         [0.5637255 , 0.5004902 , 0.37254903],\n",
      "         [0.54901963, 0.49019608, 0.37561274],\n",
      "         ...,\n",
      "         [0.43792892, 0.3529412 , 0.29761028],\n",
      "         [0.40643382, 0.3396446 , 0.26911765],\n",
      "         [0.35637254, 0.2740196 , 0.23253676]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.86262256, 0.9466299 , 0.92696077],\n",
      "         [0.93088233, 0.9476716 , 0.93192405],\n",
      "         [0.95686275, 0.9382353 , 0.9220588 ],\n",
      "         ...,\n",
      "         [0.4882353 , 0.5949755 , 0.6603554 ],\n",
      "         [0.4833946 , 0.6005515 , 0.6451593 ],\n",
      "         [0.5452819 , 0.65232843, 0.69626224]],\n",
      "\n",
      "        [[0.85036767, 0.94509804, 0.9266544 ],\n",
      "         [0.9141544 , 0.9481618 , 0.9186887 ],\n",
      "         [0.956924  , 0.9451593 , 0.9255515 ],\n",
      "         ...,\n",
      "         [0.54816175, 0.64963233, 0.7177696 ],\n",
      "         [0.49503675, 0.59748775, 0.6641544 ],\n",
      "         [0.5099265 , 0.6148284 , 0.68057597]],\n",
      "\n",
      "        [[0.86697304, 0.9356618 , 0.9178309 ],\n",
      "         [0.9284314 , 0.9490196 , 0.92401963],\n",
      "         [0.9617647 , 0.9457108 , 0.92334557],\n",
      "         ...,\n",
      "         [0.53811276, 0.62830883, 0.686152  ],\n",
      "         [0.5047181 , 0.6057598 , 0.6691789 ],\n",
      "         [0.48670343, 0.5901961 , 0.6599265 ]]],\n",
      "\n",
      "\n",
      "       [[[0.74895835, 0.84607846, 0.8303922 ],\n",
      "         [0.7259804 , 0.8409926 , 0.8181372 ],\n",
      "         [0.77683824, 0.84883577, 0.8333333 ],\n",
      "         ...,\n",
      "         [0.45232844, 0.55177695, 0.58860296],\n",
      "         [0.44454658, 0.5576593 , 0.5887255 ],\n",
      "         [0.41727942, 0.53860295, 0.5695466 ]],\n",
      "\n",
      "        [[0.7732843 , 0.8575368 , 0.8469363 ],\n",
      "         [0.7670343 , 0.8421569 , 0.8342525 ],\n",
      "         [0.73376226, 0.8613358 , 0.83480394],\n",
      "         ...,\n",
      "         [0.43927696, 0.5583946 , 0.5971201 ],\n",
      "         [0.44123775, 0.5528799 , 0.595098  ],\n",
      "         [0.42340687, 0.5316789 , 0.56960785]],\n",
      "\n",
      "        [[0.73229164, 0.85251224, 0.829473  ],\n",
      "         [0.7306985 , 0.85042894, 0.8273897 ],\n",
      "         [0.7393995 , 0.8481005 , 0.8219975 ],\n",
      "         ...,\n",
      "         [0.45539215, 0.56960785, 0.6009804 ],\n",
      "         [0.46029413, 0.5740196 , 0.60539216],\n",
      "         [0.44264707, 0.5563725 , 0.5877451 ]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.45686275, 0.4254902 , 0.3382353 ],\n",
      "         [0.43639705, 0.40753677, 0.33970588],\n",
      "         [0.44221812, 0.39123774, 0.35594362],\n",
      "         ...,\n",
      "         [0.17248775, 0.17941177, 0.15980393],\n",
      "         [0.16783088, 0.16262256, 0.1473652 ],\n",
      "         [0.18321079, 0.18321079, 0.16145833]],\n",
      "\n",
      "        [[0.428125  , 0.39969364, 0.3083946 ],\n",
      "         [0.44699755, 0.41856617, 0.34558824],\n",
      "         [0.47310048, 0.42003676, 0.36911765],\n",
      "         ...,\n",
      "         [0.18829657, 0.19221814, 0.1726103 ],\n",
      "         [0.19258578, 0.1783701 , 0.17297794],\n",
      "         [0.20330882, 0.18517157, 0.17665441]],\n",
      "\n",
      "        [[0.43229166, 0.4047794 , 0.3103554 ],\n",
      "         [0.45324755, 0.4336397 , 0.3336397 ],\n",
      "         [0.45667893, 0.4208946 , 0.34295344],\n",
      "         ...,\n",
      "         [0.17144608, 0.17144608, 0.16421568],\n",
      "         [0.17696078, 0.16930147, 0.17267157],\n",
      "         [0.19871323, 0.1903799 , 0.17910539]]],\n",
      "\n",
      "\n",
      "       [[[0.8122549 , 0.93425244, 0.9128064 ],\n",
      "         [0.844424  , 0.94344366, 0.92224264],\n",
      "         [0.8583946 , 0.9488358 , 0.9254902 ],\n",
      "         ...,\n",
      "         [0.5115809 , 0.6713848 , 0.74148285],\n",
      "         [0.51384807, 0.63854164, 0.70159316],\n",
      "         [0.50784314, 0.6343137 , 0.70643383]],\n",
      "\n",
      "        [[0.81666666, 0.9372549 , 0.90588236],\n",
      "         [0.8813726 , 0.9406863 , 0.92990196],\n",
      "         [0.83094364, 0.92898285, 0.9064338 ],\n",
      "         ...,\n",
      "         [0.48106617, 0.61427695, 0.71231616],\n",
      "         [0.49583334, 0.62218136, 0.72022057],\n",
      "         [0.46648285, 0.6007966 , 0.699326  ]],\n",
      "\n",
      "        [[0.7701593 , 0.8938113 , 0.87438726],\n",
      "         [0.82058823, 0.9397059 , 0.9219363 ],\n",
      "         [0.7882966 , 0.93780637, 0.9191789 ],\n",
      "         ...,\n",
      "         [0.50594366, 0.641299  , 0.7293505 ],\n",
      "         [0.4852941 , 0.6245098 , 0.71954656],\n",
      "         [0.43596813, 0.5928309 , 0.68694854]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.42879903, 0.3302696 , 0.28468138],\n",
      "         [0.43609068, 0.32542893, 0.2827206 ],\n",
      "         [0.438174  , 0.3180147 , 0.2966299 ],\n",
      "         ...,\n",
      "         [0.52120095, 0.5824755 , 0.64571077],\n",
      "         [0.5329044 , 0.5837622 , 0.6573529 ],\n",
      "         [0.49325982, 0.57990193, 0.6655024 ]],\n",
      "\n",
      "        [[0.41213235, 0.31960785, 0.2848652 ],\n",
      "         [0.40753677, 0.3211397 , 0.2776348 ],\n",
      "         [0.39025736, 0.32150736, 0.28284314],\n",
      "         ...,\n",
      "         [0.50398284, 0.5656863 , 0.62371325],\n",
      "         [0.53547794, 0.5731005 , 0.63884807],\n",
      "         [0.5041054 , 0.57463235, 0.64099264]],\n",
      "\n",
      "        [[0.38253677, 0.2995098 , 0.26942402],\n",
      "         [0.37254903, 0.3060662 , 0.27126226],\n",
      "         [0.3488358 , 0.31219363, 0.26875   ],\n",
      "         ...,\n",
      "         [0.48443627, 0.5648897 , 0.61942405],\n",
      "         [0.51145834, 0.55496323, 0.61605394],\n",
      "         [0.5109069 , 0.56623775, 0.6363358 ]]]], dtype=float32)>, (<tf.Tensor: shape=(10, 1), dtype=uint8, numpy=\n",
      "array([[1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1]], dtype=uint8)>, <tf.Tensor: shape=(10, 4), dtype=float16, numpy=\n",
      "array([[0.3945, 0.1674, 0.9365, 1.    ],\n",
      "       [0.3438, 0.0986, 0.8726, 0.777 ],\n",
      "       [0.3435, 0.1063, 0.953 , 0.8003],\n",
      "       [0.2751, 0.1355, 0.773 , 0.861 ],\n",
      "       [0.1526, 0.3323, 0.5527, 0.778 ],\n",
      "       [0.3347, 0.3494, 0.813 , 0.967 ],\n",
      "       [0.0694, 0.2229, 0.6245, 0.8037],\n",
      "       [0.1487, 0.3342, 0.67  , 1.    ],\n",
      "       [0.6094, 0.2607, 1.    , 0.8716],\n",
      "       [0.    , 0.1218, 0.2394, 1.    ]], dtype=float16)>))\n"
     ]
    }
   ],
   "source": [
    "for batch in train.take(1):\n",
    "    print(batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rajus\\OneDrive\\Desktop\\Rahul Project\\AIML\\myenv\\Lib\\site-packages\\keras\\src\\saving\\saving_api.py:107: UserWarning: You are saving a model that has not yet been built. It might not contain any weights yet. Consider building the model first by calling it on some data.\n",
      "  return saving_lib.save_model(model, filepath)\n"
     ]
    }
   ],
   "source": [
    "model.save(\"../Model/model.keras\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
